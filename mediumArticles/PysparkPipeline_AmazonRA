Pyspark Pipeline: Malformed Excel Worksheet to ML Pipeline and Prediction storage to Database

Objective: To discuss the implementation of the Pyspark pipeline with a realworld data. Start from the very beginning.
- Diving into real world file formats.(Ignore it
- XL file provided by third-party, 
- File is read into temporary Pandas dataframe,
- Data from worksheet is extracted into multiple dataframes,
- Pandas DF is converted to Pyspark DF
- PysparkDF is written to the Database (Output 1)
- Data is analysed and data manipulated for Data visualisation and ML model, 
- ML pipeline is created using MLlib 
- Prediction dataframe is returned.
- Dataframe is written to the Database (Output 2)

SideEffect: I explore Pyspark ecosystem to find some unique concepts that were missed. Especially the concepts the data engineers and those working in data must
know in detail. Writing this post cleared many doubts I had. The error thrown by JDBC driver, the function createDataFrame and using the RegressionEvaluator module were learnt when the post was written. 
 
Real world Files:

When you want to do real-world data analysis, have you been overwhelmed by the number of files that needs to be ingested, or confused with the multiple tables inside the excel worksheet shared with you? 
1) The files and worksheets are reflection of the thinking method the creator has. People think in a non-linear, and inter-connected way, and the tools like excel works as a canvas. That results in multiple tables inside a single worksheet handed over to you.

2) Another common challenge is to have huge number of files in individual subfolders. The file format may be text/csv/json or any of the compressed formats out there. The data inside these files are organised into individual tables, with its own column names and schema. These files are created from a database, an application or another program. You must have guessed, the file creator's mind has been forced to use a set of formats.

3) Then comes the ubiquitous text files, that either have extensions or simply just a name. Webscraping data, Logfiles, ML models, configuration files, output of devices, blog posts, ascii arts(https://en.wikipedia.org/wiki/ASCII_art) etc comes unders this category. Machines and some humans have fetish for files without extensions. Those of us, who have used used notepad to create text file might think, these files cannot become complicated. Text files, due to its freedom, can contain data in a very convoluted fashion, or might have structured formats like Json/ CSV written in wrong format. Such formats when created by machines or programs makes me think that programs were faulty, however the reality is, the programmmer mind was entangled in multiple priorities.Dealing with text files in Pyspark is simplified too, but will be taken up in a seperate post

RDDs and RDD Dataframes:
Pyspark has excellent support for the more structured data, even when the files are nested inside multiple folders. By default pyspark does recursiveFileLookup when the folder path is provided. The read method in SparkSession has following arguments that are frequently used in reading in the file. In this case "CSV" file format. Notice the "recursiveFileLookup" argument which is a booleanType. So if you  

Signature:
sparkSession.read.csv(
    path: Union[str, List[str]],
    schema: Union[pyspark.sql.types.StructType, str, NoneType] = None,
    sep: Optional[str] = None,
    header: Union[bool, str, NoneType] = None,
    inferSchema: Union[bool, str, NoneType] = None,
    recursiveFileLookup: Union[bool, str, NoneType] = None,
) -> 'DataFrame'

The unstructed variety of the files can be processed with Pyspark RDDs. Which can be read into the Spark Context using  sparkContext.textFile(filePath). 

Signature:
sparkContext.textFile(
    name: str,
    minPartitions: Optional[int] = None,
    use_unicode: bool = True,
) -> pyspark.rdd.RDD[str]

Notice the object type from each method. The former is a Dataframe, while the later is a RDD. I will let you to explore reading in the structured and unstructured file format into Pyspark session, and see how the data inside the objects are structured. Takes some time to get used to the RDD object. I had discussed about manipulating RDDs with map-filter-reduce option here. It can jumpstart your exploration

Then why fallback to Pandas?
To uphold the DRY principle. When someone has already written much of the clean up functions and attached it to pandas dataframe objects as methods, it is inefficient to re-invent the wheels. Ensure Pandas and Openpyxl libraries are installed. The file reading is done as below. The methods you will need from pandas are

1) pd.read_excel
2) ds.columns
3) ds.dropna(axis=0/1,inplace=True)
4) ds.iloc[1:,:]
5) ds[[columns,that,you,need]]

ds= pd.read_excel("sales Target Dashboard.xlsx",sheet_name="DataSource",
                          parse_dates=True)
                          
Start by looking at the columns that are read into the dataframe using ds.columns. Which might show outputs that contains named columns mixed with unnamed columns. The unnamed columns usually contain null values, so safely dropped. But in case of excel worksheets, scrutinize the dataframe more closely.

Index(['S/N', 'Date', 'Branch', 'Pizza Type', 'Quantity', 'Time', 'Time Range',
       'Price', 'Unnamed: 11','Unnamed: 12', 'Daily Target', 'Unnamed: 14', 
       'Sales Target','Unnamed: 19','Branch.1', 'Unnamed: 24',
       'Unnamed: 25'],
      dtype='object')

The data inside the worksheets are like islands seperated by ocean of empty cells. Use the iloc[...] and ds[[column,names]] commands to cut out the columns and rows that have some data inside it and discard the empty cells later by using the ds.dropna() method. After this activity, you will have multiple dataframes, with clean data inside. The columns of the dataframes will look something like below

saledata.columns => ['S/N', 'Date', 'Branch', 'Pizza Type', 'Quantity', 'Time', 'Time Range','Price']
branchdata.columns =>['DailyTarget', 'SalesTarget']
branchtarget.columns => ['Branch', 'Manager', 'Location']

Pandas to Pyspark world:

To move from pandas to pyspark world, the magic command is,

sparkDF = sparkSession.createDataFrame(pandas_dataframe_object)

That will convert the pandas dataframe, and extend the SQL and Spark methods for processing data. I am addicted with the functionality SQL functions provide. On top of it the functions can be chained inside the select statement, as if writing regular sql query. It is very intuitive to work with, and easy to learn. The pre-requisite is you have practiced your SQL chops. There is a very good reason for you to master SQL, now. 

Writing out the dataframe to CSV file and to Database:

Pyspark dataframes can be written to csv files to the local folder and it can be written out directly to Database, if 
- Appropriate JDBC drivers are available 
- Spark session has been configured to use the drivers and then started. 
- Pyspark dataframe is created using thus configured spark session. 

If any of the above steps is missed, then writing to the database will error out( okay, throw an exception!!!)

The below function takes the pyspark dataframe, and pushes it directly to the database of your choice, with the tableName of your choice.

```
def writingSparkDFtoDatabase(sparkDF,dbName,dbTableName):
    
    try:
        sparkDF.write \
                    .format('jdbc') \
                    .option("url", f"jdbc:postgresql://localhost:5432/{dbName}") \
                    .option('dbtable', dbTableName) \
                    .option('user','postgres') \
                    .option('password', 1234) \
                    .option('driver','org.postgresql.Driver') \
                    .save(mode='overwrite')
        print('Write Complete')
    except Exception as e:
        print(f'Write errored out due to {e}')
```

To write the dataframe to csv file following signature is used. And note, the path has to be "folder name". Pyspark writes multiple files to the folder, since pyspark works in a distributed environment.

psDataframeObject.write.csv(
    path: str,
    mode: Optional[str] = None,
    compression: Optional[str] = None,
    header: Union[bool, str, NoneType] = None,
) -> None

Time for Truth about Pipelines:

There are two pipelines that is used in data processing. The data ingestion and extraction pipeline that I have discussed above. Another pipeline is, actually an Object inside Pyspark environment. The pipeline of the Object kind is invoked by calling the Pipeline function

from pyspark.ml import PipeLine

We will see this Pipeline in Action. The sales_data that we cleaned up contains the following columns. 

+---+-----+------+------------+--------+-----------------+-------------+-----+
|S/N| Date|Branch|  Pizza Type|Quantity|             Time|   Time Range|Price|
+---+-----+------+------------+--------+-----------------+-------------+-----+
|  1|42349| Abuja|     Meatzaa|       5|0.333344907407407|Before 9:00am|    8|
|  2|42352|Ibadan|Extravaganza|       4|0.333356481481482|Before 9:00am|    8|
+---+-----+------+------------+--------+-----------------+-------------+-----+
only showing top 2 rows


Lets predict the quantity of pizza, given the branch, pizza name, time. The columns needs to be selected, the stringindexer encoding needs to be applied for the pizza type, and branch to convert it to numbers. Then apply the regression model on the features, with quantity as "label/target" column

Predicting the quantity of Pizza:

>> Import
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.regression import LinearRegression

>> Initialising functions

saleStringer = StringIndexer(inputCols=["Branch","Pizza Type"],
                            outputCols=["Branch_idx","Pizza_idx"])

vectors = VectorAssembler(inputCols=["Branch_idx","Pizza_idx","Time"],
                         outputCol="features")

linReg = LinearRegression(featuresCol='features',labelCol="Quantity")

#Pipeline is created with indexer, assembler and regressor

pipeSales = Pipeline(stages=[saleStringer,vectors,linReg])

>> Start Modeling & Predicting:

salesModel = pipeSales.fit(train)
quantityPredict = salesModel.transform(test)

quantityPredict.select("Quantity","Prediction","Branch","Pizza Type").show()

+--------+------------------+--------+----------------+
|Quantity|        Prediction|  Branch|      Pizza Type|
+--------+------------------+--------+----------------+
|       5| 3.050774961385897|   Ikoyi|     BBQ Chicken|
|       4|2.9801379485860604|   Lekki|         Meatzaa|
|       5|3.0190306653855288|   Abuja|BBQ Philly Steak|
|       5|3.0223243453975925|   Ikoyi|  Chicken Legend|
|       5| 3.014884106037318|   Abuja|   Chicken Feast|
|       5|3.0384103223998515|   Ikoyi|       Beef Suya|

There is the prediction data for each pizza. The data is available as pyspark dataframe, which can be written to the database for front-end team to process further.Ensure the 'features' columns generated by the VectorAssembler is left behind, else the jdbc driver will throw exception.

writingSparkDFtoDatabase(predictionDF,'dashboards','quantityprediction')

There is still one final step, which the Data Scientists have to complete. That is evaluating the model. This step can be clubbed with CI/CD pipeline for testing the complete Data Ingestion and ML pipelines. 

>> Evaluating the model:

from pyspark.ml.evaluation import RegressionEvaluator

salEval = RegressionEvaluator(labelCol="Quantity")
salEval.evaluate(quantityPredict)

print(salEval.getThroughOrigin())
print(salEval.evaluate(quantityPredict, {salEval.metricName: "mae"}))
print(salEval.evaluate(quantityPredict, {salEval.metricName: "r2"}))

Wrap Up:

That's all folks, the ML pipeline. In fact, the two lines, fit & transform followed by Evaluation step is where all the Data Science lies. Rest is all Data Engineering. In DS also, there is lot more knobs to tweak, and much theory. All of that has been simply abstracted.If you felt DS related activity is very less, then it is normal. The value added by Data Scientist is, his ability to understand the math and how it related to real world. While the Data Engineer brings the Data, and the real world information to the door steps of Data Scientist.  

