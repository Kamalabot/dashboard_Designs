{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a1cf49d",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "To import the Amazon dataset into the spark environment\n",
    "\n",
    "Do minimum neccary Data analysis of the data\n",
    "\n",
    "Create features, labels dataset using the pipeline, including the transformer, estimator and finally evaluator\n",
    "\n",
    "Write the final model and the prediction to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f89d84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting with import of pyspark and related modules\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pyspark.ml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa5be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mllibPath = \"mllib/\"\n",
    "externalData = \"externalData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9183d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.unpack_archive(externalData+\"amazon-business-research-analyst-dataset.zip\",extract_dir=externalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37bb0f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 05:17:15 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 172.17.0.1 instead (on interface docker0)\n",
      "22/11/29 05:17:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/11/29 05:17:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "    \n",
    "sparkSQL = SparkSession.builder.appName('amazonRA') \\\n",
    "        .config('spark.jars',\"/usr/share/java/postgresql-42.2.26.jar\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6244fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkReader = sparkSQL.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7087ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkContext = sparkSQL.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d1d95c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aidData.csv\n",
      "amazon-business-research-analyst-dataset.zip\n",
      "athletes.csv\n",
      "bls-industry-unemployment.csv\n",
      "bls-metro-unemployment.csv\n",
      "bseScripts.json\n",
      "cleaned_test.csv\n",
      "Common_FMCG_Labelled_Quarter_Results.csv\n",
      "companyManagement.csv\n",
      "companyMarketData.csv\n",
      "Electricity generation.csv\n",
      "encoded_cleaned_test.csv\n",
      "googleMerchPurchases.csv\n",
      "Laser_Report_2020.xlsx\n",
      "nyc-collisions-2019-reduced.csv\n",
      "protests.csv\n",
      "purchase_data.csv\n",
      "Quora_answers.csv\n",
      "selected-indicators-from-world-bank-20002019.zip\n",
      "skillshare-top-1000-course.zip\n",
      "stocks.csv\n",
      "top-pypi-packages-30-days.json\n",
      "unemployment.csv\n",
      "updated.csv\n",
      "us-congress-members.csv\n",
      "vizheads.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cd externalData/\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f53ccf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_RA = sparkReader.csv(externalData+\"cleaned_test.csv\",\n",
    "                                  inferSchema=True,\n",
    "                                  header=True,\n",
    "                                 sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67c3a494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Delivery_person_ID: string (nullable = true)\n",
      " |-- Delivery_person_Age: double (nullable = true)\n",
      " |-- Delivery_person_Ratings: double (nullable = true)\n",
      " |-- Restaurant_latitude: double (nullable = true)\n",
      " |-- Restaurant_longitude: double (nullable = true)\n",
      " |-- Delivery_location_latitude: double (nullable = true)\n",
      " |-- Delivery_location_longitude: double (nullable = true)\n",
      " |-- Order_Date: string (nullable = true)\n",
      " |-- Time_Orderd: string (nullable = true)\n",
      " |-- Time_Order_picked: string (nullable = true)\n",
      " |-- Weather: string (nullable = true)\n",
      " |-- Road_traffic_density: string (nullable = true)\n",
      " |-- Vehicle_condition: integer (nullable = true)\n",
      " |-- Type_of_order: string (nullable = true)\n",
      " |-- Type_of_vehicle: string (nullable = true)\n",
      " |-- multiple_deliveries: double (nullable = true)\n",
      " |-- Festival: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Name:: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_RA.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c15b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "deliveryPersonFact = cleaned_RA.select(\"Delivery_person_ID\",\"Delivery_person_Age\",\n",
    "                 \"Delivery_person_Ratings\",\"Type_of_Vehicle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6ad7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersFact = cleaned_RA.select(\"Restaurant_latitude\",\"Restaurant_longitude\",\n",
    "                              \"Delivery_location_latitude\",\"Delivery_location_longitude\",\n",
    "                              \"Order_Date\",\"Time_Orderd\",\"Time_Order_picked\",\"Type_of_order\",\n",
    "                              \"multiple_deliveries\",\"Road_traffic_density\",\"Weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc04f95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-------+\n",
      "|Type_of_order|Road_traffic_density|Weather|\n",
      "+-------------+--------------------+-------+\n",
      "|       Drinks|                 NaN|    NaN|\n",
      "|        Snack|                 Jam|  Windy|\n",
      "|       Drinks|                 Jam| Stormy|\n",
      "|         Meal|              Medium|    Fog|\n",
      "|       Drinks|              Medium|  Sunny|\n",
      "|       Drinks|                 Low|    Fog|\n",
      "|       Buffet|                 Low|  Windy|\n",
      "|         Meal|              Medium|  Windy|\n",
      "|        Snack|                 Jam| Cloudy|\n",
      "|         Meal|                 Jam|    Fog|\n",
      "|        Snack|                High| Stormy|\n",
      "|       Drinks|                 Low|  Windy|\n",
      "|        Snack|                 Low|  Sunny|\n",
      "|       Drinks|                 Low|  Windy|\n",
      "|         Meal|                 Jam|    Fog|\n",
      "|        Snack|                 Jam| Cloudy|\n",
      "|       Buffet|                 Jam|  Windy|\n",
      "|        Snack|                 Jam|  Sunny|\n",
      "|        Snack|              Medium| Cloudy|\n",
      "|       Drinks|                 Jam| Stormy|\n",
      "+-------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersFact.select(\"Type_of_order\",\"Road_traffic_density\",\"Weather\") \\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcefb7a1",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "To learn about the various transformers available in pyspark and implement a couple in Amazon analysis\n",
    "\n",
    "Learn about the multiple ML algorithms, and use atleast Regression, Classification and Clustering on Amazon RA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e5662c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-----------------------+----------------+\n",
      "|Delivery_person_ID|Delivery_person_Age|Delivery_person_Ratings| Type_of_Vehicle|\n",
      "+------------------+-------------------+-----------------------+----------------+\n",
      "|   COIMBRES13DEL01|                NaN|                    NaN|electric_scooter|\n",
      "|    BANGRES15DEL01|               28.0|                    4.6|      motorcycle|\n",
      "+------------------+-------------------+-----------------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deliveryPersonFact.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af239c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first use the imputer to update the Null values.\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "deliveryImputer = Imputer(inputCols=[\"Delivery_person_Age\",\n",
    "                                     \"Delivery_person_Ratings\"],\n",
    "                         outputCols=[\"DeliveryPersonAge\",\n",
    "                                    \"DeliveryPersonRatings\"],\n",
    "                         strategy=\"mean\")\n",
    "\n",
    "deliveryModel = deliveryImputer.fit(deliveryPersonFact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbe2d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "deliveryPersonNullRemoved = deliveryModel.transform(deliveryPersonFact). \\\n",
    "                drop(\"Delivery_person_Age\",\"Delivery_person_Ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7568a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "deliveryPersonNullRemoved = deliveryPersonNullRemoved.select(\"Delivery_person_ID\",\"Type_of_Vehicle\",\n",
    "                                round(\"DeliveryPersonAge\",1).alias(\"DeliveryPersonAge\"),\n",
    "                                round(\"DeliveryPersonRatings\",1).alias(\"DeliveryPersonRatings\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d766fdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+-----------------+---------------------+\n",
      "|Delivery_person_ID| Type_of_Vehicle|DeliveryPersonAge|DeliveryPersonRatings|\n",
      "+------------------+----------------+-----------------+---------------------+\n",
      "|   COIMBRES13DEL01|electric_scooter|             29.5|                  4.6|\n",
      "|    BANGRES15DEL01|      motorcycle|             28.0|                  4.6|\n",
      "|     JAPRES09DEL03|      motorcycle|             23.0|                  4.5|\n",
      "|     JAPRES07DEL03|         scooter|             21.0|                  4.8|\n",
      "|    CHENRES19DEL01|         scooter|             31.0|                  4.6|\n",
      "+------------------+----------------+-----------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deliveryPersonNullRemoved.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fde8eaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "| Type_of_vehicle|\n",
      "+----------------+\n",
      "|      motorcycle|\n",
      "|         scooter|\n",
      "|electric_scooter|\n",
      "|         bicycle|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Next we will work on encoding the type of vehicles\n",
    "deliveryPersonNullRemoved.select(\"Type_of_vehicle\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e98be03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+-----------------+---------------------+-----------------+\n",
      "|Delivery_person_ID| Type_of_Vehicle|DeliveryPersonAge|DeliveryPersonRatings|EncodedCategories|\n",
      "+------------------+----------------+-----------------+---------------------+-----------------+\n",
      "|   COIMBRES13DEL01|electric_scooter|             29.5|                  4.6|              2.0|\n",
      "|    BANGRES15DEL01|      motorcycle|             28.0|                  4.6|              0.0|\n",
      "+------------------+----------------+-----------------+---------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#There are three modules to use, OneHotEncoder, VectorIndexer, StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorIndexer, StringIndexer\n",
    "stringIndexModel = StringIndexer(inputCol=\"Type_of_Vehicle\",outputCol=\"EncodedCategories\")\n",
    "delivery_stringIndexed = stringIndexModel.fit(deliveryPersonNullRemoved).transform(deliveryPersonNullRemoved)\n",
    "delivery_stringIndexed.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9c8d2864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|EncodedCategories|\n",
      "+-----------------+\n",
      "|              0.0|\n",
      "|              1.0|\n",
      "|              3.0|\n",
      "|              2.0|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_stringIndexed.select(\"EncodedCategories\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30283692",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexOrder = StringIndexer(inputCols=[\"Weather\",\"Road_traffic_density\",\n",
    "                                            \"Type_of_order\",\"multiple_deliveries\"]\n",
    "                                 ,outputCols=[\"Weather_idx\",\"Road_traffic_density_idx\",\n",
    "                                            \"Type_of_order_idx\",\"multiple_deliveries_idx\"])\n",
    "order_stringIndexed = stringIndexOrder.fit(ordersFact).transform(ordersFact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "36a6cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_stringIndexed = order_stringIndexed.drop(\"Weather\",\"Road_traffic_density\",\n",
    "                                            \"Type_of_order\",\"multiple_deliveries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a4b40e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cast in module typing:\n",
      "\n",
      "cast(typ, val)\n",
      "    Cast a value to a type.\n",
      "    \n",
      "    This returns the value unchanged.  To the type checker this\n",
      "    signals that the return value has the designated type, but at\n",
      "    runtime we intentionally don't check anything (we want this\n",
      "    to be as fast as possible).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ba3a8ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+-----------------------------------------------------------------------+\n",
      "|Time_Order_picked|Time_orderd|(CAST(Time_Order_picked AS TIMESTAMP) - CAST(Time_orderd AS TIMESTAMP))|\n",
      "+-----------------+-----------+-----------------------------------------------------------------------+\n",
      "|            15:05|        NaN|                                                                   null|\n",
      "|            20:35|      20:30|                                                   INTERVAL '0 00:05...|\n",
      "|            19:45|      19:35|                                                   INTERVAL '0 00:10...|\n",
      "|            17:20|      17:15|                                                   INTERVAL '0 00:05...|\n",
      "|            18:40|      18:25|                                                   INTERVAL '0 00:15...|\n",
      "+-----------------+-----------+-----------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_stringIndexed.select(\"Time_Order_picked\",\"Time_orderd\",\n",
    "                           col(\"Time_Order_picked\").cast(\"timestamp\") - col(\"Time_orderd\").cast(\"timestamp\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e50c4cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|timeOrderFormated|OrderPickedFormated|\n",
      "+-----------------+-------------------+\n",
      "|             null|              15:11|\n",
      "|            20:11|              20:11|\n",
      "|            19:11|              19:11|\n",
      "|            17:11|              17:11|\n",
      "|            18:11|              18:11|\n",
      "|            09:11|              09:11|\n",
      "|             null|              10:11|\n",
      "|             null|              18:11|\n",
      "|            21:11|              21:11|\n",
      "|            20:11|              20:11|\n",
      "|            14:11|              14:11|\n",
      "|            23:11|              23:11|\n",
      "|            22:11|              22:11|\n",
      "|            23:11|              23:11|\n",
      "|            21:11|              21:11|\n",
      "|            20:11|              20:11|\n",
      "|            20:11|              20:11|\n",
      "|            21:11|              22:11|\n",
      "|            18:11|              18:11|\n",
      "|            21:11|              21:11|\n",
      "+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_stringIndexed. \\\n",
    "        select(date_format(\"Time_orderd\",'HH:MM').alias(\"timeOrderFormated\"),\n",
    "              date_format(\"Time_Order_picked\",'HH:MM').alias(\"OrderPickedFormated\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75ad9907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writingCSVFiletoDatabase(session, csvFile,dbName,dbTableName):\n",
    "    \n",
    "    fileSparkDF = session.read.csv(csvFile,inferSchema=True,header=True)\n",
    "    try:\n",
    "        fileSparkDF.write \\\n",
    "                    .format('jdbc') \\\n",
    "                    .option(\"url\", f\"jdbc:postgresql://localhost:5432/{dbName}\") \\\n",
    "                    .option('dbtable', dbTableName) \\\n",
    "                    .option('user','postgres') \\\n",
    "                    .option('password', 1234) \\\n",
    "                    .option('driver','org.postgresql.Driver') \\\n",
    "                    .save(mode='overwrite')\n",
    "        print('Write Complete')\n",
    "    except Exception as e:\n",
    "        print(f'Write errored out due to {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4424088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writingSparkDFtoDatabase(session,sparkDF,dbName,dbTableName):\n",
    "    \n",
    "    try:\n",
    "        sparkDF.write \\\n",
    "                    .format('jdbc') \\\n",
    "                    .option(\"url\", f\"jdbc:postgresql://localhost:5432/{dbName}\") \\\n",
    "                    .option('dbtable', dbTableName) \\\n",
    "                    .option('user','postgres') \\\n",
    "                    .option('password', 1234) \\\n",
    "                    .option('driver','org.postgresql.Driver') \\\n",
    "                    .save(mode='overwrite')\n",
    "        print('Write Complete')\n",
    "    except Exception as e:\n",
    "        print(f'Write errored out due to {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6eddfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Complete\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba9550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
