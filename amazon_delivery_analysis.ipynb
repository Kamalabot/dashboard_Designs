{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a1cf49d",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "To import the Amazon dataset into the spark environment : **Done**\n",
    "\n",
    "Do minimum neccary Data analysis of the data : **Done**\n",
    "\n",
    "Create features, labels dataset using the pipeline, including the transformer, estimator and finally evaluator : **pipeline is yet to be done, rest is complete**\n",
    "\n",
    "Write the final model and the prediction to the database : **done**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f89d84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting with import of pyspark and related modules\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pyspark.ml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa5be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mllibPath = \"mllib/\"\n",
    "externalData = \"externalData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9183d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.unpack_archive(externalData+\"amazon-business-research-analyst-dataset.zip\",extract_dir=externalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37bb0f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 04:08:57 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 172.17.0.1 instead (on interface docker0)\n",
      "22/11/30 04:08:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/11/30 04:08:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "    \n",
    "sparkSQL = SparkSession.builder.appName('amazonRA') \\\n",
    "        .config('spark.jars',\"/usr/share/java/postgresql-42.2.26.jar\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6244fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkReader = sparkSQL.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7087ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkContext = sparkSQL.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d1d95c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aidData.csv\n",
      "amazon-business-research-analyst-dataset.zip\n",
      "athletes.csv\n",
      "bls-industry-unemployment.csv\n",
      "bls-metro-unemployment.csv\n",
      "bseScripts.json\n",
      "cleaned_test.csv\n",
      "Common_FMCG_Labelled_Quarter_Results.csv\n",
      "companyManagement.csv\n",
      "companyMarketData.csv\n",
      "Electricity generation.csv\n",
      "encoded_cleaned_test.csv\n",
      "googleMerchPurchases.csv\n",
      "Laser_Report_2020.xlsx\n",
      "nyc-collisions-2019-reduced.csv\n",
      "protests.csv\n",
      "purchase_data.csv\n",
      "Quora_answers.csv\n",
      "selected-indicators-from-world-bank-20002019.zip\n",
      "skillshare-top-1000-course.zip\n",
      "stocks.csv\n",
      "top-pypi-packages-30-days.json\n",
      "unemployment.csv\n",
      "updated.csv\n",
      "us-congress-members.csv\n",
      "vizheads.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cd externalData/\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f53ccf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_import = sparkReader.csv(externalData+\"cleaned_test.csv\",\n",
    "                                  inferSchema=True,\n",
    "                                  header=True,\n",
    "                                 sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb26de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating row number for data\n",
    "from pyspark.sql.window import Window\n",
    "row_w = Window().orderBy(lit('ID'))\n",
    "cleaned_RA = cleaned_import.withColumn(\"rowNum\",row_number().over(row_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67c3a494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Delivery_person_ID: string (nullable = true)\n",
      " |-- Delivery_person_Age: double (nullable = true)\n",
      " |-- Delivery_person_Ratings: double (nullable = true)\n",
      " |-- Restaurant_latitude: double (nullable = true)\n",
      " |-- Restaurant_longitude: double (nullable = true)\n",
      " |-- Delivery_location_latitude: double (nullable = true)\n",
      " |-- Delivery_location_longitude: double (nullable = true)\n",
      " |-- Order_Date: string (nullable = true)\n",
      " |-- Time_Orderd: string (nullable = true)\n",
      " |-- Time_Order_picked: string (nullable = true)\n",
      " |-- Weather: string (nullable = true)\n",
      " |-- Road_traffic_density: string (nullable = true)\n",
      " |-- Vehicle_condition: integer (nullable = true)\n",
      " |-- Type_of_order: string (nullable = true)\n",
      " |-- Type_of_vehicle: string (nullable = true)\n",
      " |-- multiple_deliveries: double (nullable = true)\n",
      " |-- Festival: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Name:: string (nullable = true)\n",
      " |-- rowNum: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_RA.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "816596e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedRA = cleaned_RA.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c15b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "deliveryPersonFact = cleanedRA.select(\"ID\",\"Delivery_person_ID\",\"Delivery_person_Age\",\n",
    "                 \"Delivery_person_Ratings\",\"Type_of_Vehicle\",\"rowNum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d81b95f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 04:14:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/11/30 04:14:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/11/30 04:14:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/11/30 04:14:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/11/30 04:14:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+------+------------------+-------------------+-----------------------+---------------+------+\n",
      "|    ID|Delivery_person_ID|Delivery_person_Age|Delivery_person_Ratings|Type_of_Vehicle|rowNum|\n",
      "+------+------------------+-------------------+-----------------------+---------------+------+\n",
      "|0x3474|    BANGRES15DEL01|               28.0|                    4.6|     motorcycle|     2|\n",
      "|0x9420|     JAPRES09DEL03|               23.0|                    4.5|     motorcycle|     3|\n",
      "+------+------------------+-------------------+-----------------------+---------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deliveryPersonFact.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89e98fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 04:25:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/11/30 04:25:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/11/30 04:25:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/11/30 04:25:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/11/30 04:25:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+------+------------------+-------------------+-----------------------+---------------+------+--------+\n",
      "|    ID|Delivery_person_ID|Delivery_person_Age|Delivery_person_Ratings|Type_of_Vehicle|rowNum| dirName|\n",
      "+------+------------------+-------------------+-----------------------+---------------+------+--------+\n",
      "|0x3474|    BANGRES15DEL01|               28.0|                    4.6|     motorcycle|     2|cafolder|\n",
      "|0x9420|     JAPRES09DEL03|               23.0|                    4.5|     motorcycle|     3|cafolder|\n",
      "+------+------------------+-------------------+-----------------------+---------------+------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deliveryPersonFact.withColumn(\"dirName\",lit(\"cafolder\")).select(\"*\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a6ad7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersFact = cleanedRA.select(col(\"ID\").alias(\"order_ID\"),\"Restaurant_latitude\",\"Restaurant_longitude\",\n",
    "                              \"Delivery_location_latitude\",\"Delivery_location_longitude\",\n",
    "                              \"Order_Date\",\"Time_Orderd\",\"Time_Order_picked\",\"Type_of_order\",\n",
    "                              \"multiple_deliveries\",\"Road_traffic_density\",\"Weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcefb7a1",
   "metadata": {},
   "source": [
    "### Learning Objective\n",
    "\n",
    "To learn about the various transformers available in pyspark and implement a couple in Amazon analysis\n",
    "\n",
    "Learn about the multiple ML algorithms, and use atleast Regression, Classification and Clustering on Amazon RA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9e5662c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-------------------+-----------------------+---------------+\n",
      "|    ID|Delivery_person_ID|Delivery_person_Age|Delivery_person_Ratings|Type_of_Vehicle|\n",
      "+------+------------------+-------------------+-----------------------+---------------+\n",
      "|0x3474|    BANGRES15DEL01|               28.0|                    4.6|     motorcycle|\n",
      "|0x9420|     JAPRES09DEL03|               23.0|                    4.5|     motorcycle|\n",
      "+------+------------------+-------------------+-----------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deliveryPersonFact.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "af239c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first use the imputer to update the Null values.\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "deliveryImputer = Imputer(inputCols=[\"Delivery_person_Age\",\n",
    "                                     \"Delivery_person_Ratings\"],\n",
    "                         outputCols=[\"DeliveryPersonAge\",\n",
    "                                    \"DeliveryPersonRatings\"],\n",
    "                         strategy=\"mean\")\n",
    "\n",
    "deliveryModel = deliveryImputer.fit(deliveryPersonFact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fbe2d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "deliveryPersonNullRemoved = deliveryModel.transform(deliveryPersonFact). \\\n",
    "                drop(\"Delivery_person_Age\",\"Delivery_person_Ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c7568a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "deliveryPersonNullRemoved = deliveryPersonNullRemoved.select(\"ID\",\"Delivery_person_ID\",\"Type_of_Vehicle\",\n",
    "                                round(\"DeliveryPersonAge\",1).alias(\"DeliveryPersonAge\"),\n",
    "                                round(\"DeliveryPersonRatings\",1).alias(\"DeliveryPersonRatings\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d766fdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+---------------+-----------------+---------------------+\n",
      "|    ID|Delivery_person_ID|Type_of_Vehicle|DeliveryPersonAge|DeliveryPersonRatings|\n",
      "+------+------------------+---------------+-----------------+---------------------+\n",
      "|0x3474|    BANGRES15DEL01|     motorcycle|             28.0|                  4.6|\n",
      "|0x9420|     JAPRES09DEL03|     motorcycle|             23.0|                  4.5|\n",
      "|0x72ee|     JAPRES07DEL03|        scooter|             21.0|                  4.8|\n",
      "|0xa759|    CHENRES19DEL01|        scooter|             31.0|                  4.6|\n",
      "|0xc4af|     GOARES04DEL01|     motorcycle|             26.0|                  4.7|\n",
      "+------+------------------+---------------+-----------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deliveryPersonNullRemoved.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fde8eaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "| Type_of_vehicle|\n",
      "+----------------+\n",
      "|      motorcycle|\n",
      "|         scooter|\n",
      "|electric_scooter|\n",
      "|         bicycle|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Next we will work on encoding the type of vehicles\n",
    "deliveryPersonNullRemoved.select(\"Type_of_vehicle\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e98be03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+---------------+-----------------+---------------------+-------------+----------------+\n",
      "|    ID|Delivery_person_ID|Type_of_Vehicle|DeliveryPersonAge|DeliveryPersonRatings|TypeOfVehicle|DeliveryPersonID|\n",
      "+------+------------------+---------------+-----------------+---------------------+-------------+----------------+\n",
      "|0x3474|    BANGRES15DEL01|     motorcycle|             28.0|                  4.6|          0.0|           206.0|\n",
      "|0x9420|     JAPRES09DEL03|     motorcycle|             23.0|                  4.5|          0.0|           464.0|\n",
      "+------+------------------+---------------+-----------------+---------------------+-------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#There are three modules to use, OneHotEncoder, VectorIndexer, StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorIndexer, StringIndexer\n",
    "stringIndexModel = StringIndexer(inputCols=[\"Type_of_Vehicle\",\"Delivery_person_ID\"],\n",
    "                                 outputCols=[\"TypeOfVehicle\",\"DeliveryPersonID\"])\n",
    "delivery_stringIndexed = stringIndexModel.fit(deliveryPersonNullRemoved).transform(deliveryPersonNullRemoved)\n",
    "delivery_stringIndexed.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "30283692",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexOrder = StringIndexer(inputCols=[\"Weather\",\"Road_traffic_density\",\n",
    "                                            \"Type_of_order\",\"multiple_deliveries\"]\n",
    "                                 ,outputCols=[\"Weather_idx\",\"Road_traffic_density_idx\",\n",
    "                                            \"Type_of_order_idx\",\"multiple_deliveries_idx\"])\n",
    "order_stringIndexed = stringIndexOrder.fit(ordersFact).transform(ordersFact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "36a6cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_stringIndexed = order_stringIndexed.drop(\"Weather\",\"Road_traffic_density\",\n",
    "                                            \"Type_of_order\",\"multiple_deliveries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a4b40e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-----------------+-----------+------------------------+-----------------+-----------------------+\n",
      "|order_ID|Restaurant_latitude|Restaurant_longitude|Delivery_location_latitude|Delivery_location_longitude|Order_Date|Time_Orderd|Time_Order_picked|Weather_idx|Road_traffic_density_idx|Type_of_order_idx|multiple_deliveries_idx|\n",
      "+--------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-----------------+-----------+------------------------+-----------------+-----------------------+\n",
      "|  0x3474|          12.975377|           77.696664|                 13.085377|                  77.806664|29-03-2022|      20:30|            20:35|        1.0|                     1.0|              2.0|                    0.0|\n",
      "|  0x9420|          26.911378|           75.789034|                 27.001378|                  75.879034|10-03-2022|      19:35|            19:45|        4.0|                     1.0|              0.0|                    0.0|\n",
      "+--------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-----------------+-----------+------------------------+-----------------+-----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_stringIndexed.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e3749",
   "metadata": {},
   "source": [
    "Casting the time into \"timestamp\" is simple with cast function. Once that is done, getting the interval between the time stamps is \n",
    "done using the regular subtraction. The result is interval timestamp, which can be re-casted in \"int\", which gives results in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ba3a8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickTimeDF = order_stringIndexed.select(col(\"order_ID\").alias('pick_ID'),\"Time_Order_picked\",\"Time_orderd\",\n",
    "                           (col(\"Time_Order_picked\").cast(\"timestamp\") - \n",
    "                           col(\"Time_orderd\").cast(\"timestamp\")).cast('int').alias('IntervalPickup'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e50c4cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------+--------------+\n",
      "|pick_ID|Time_Order_picked|Time_orderd|IntervalPickup|\n",
      "+-------+-----------------+-----------+--------------+\n",
      "| 0x3474|            20:35|      20:30|           300|\n",
      "| 0x9420|            19:45|      19:35|           600|\n",
      "| 0x72ee|            17:20|      17:15|           300|\n",
      "| 0xa759|            18:40|      18:25|           900|\n",
      "| 0xc4af|             9:55|       9:45|           600|\n",
      "+-------+-----------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pickTimeDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4f267f",
   "metadata": {},
   "source": [
    "The order pick up time can be predicted based on the variety of features available. The next steps will be to create the feature/label dataset using the Amazon RA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fad9ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblingData = delivery_stringIndexed.join(order_stringIndexed, delivery_stringIndexed.ID == order_stringIndexed.order_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a5fad7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblingData_1 = assemblingData.drop('order_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fcecb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblingData_2 = assemblingData_1.join(pickTimeDF,assemblingData_1.ID == pickTimeDF.pick_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "791a0653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+---------------+-----------------+---------------------+-------------+----------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-----------+------------------------+-----------------+-----------------------+-----------+--------------+\n",
      "|    ID|Delivery_person_ID|Type_of_Vehicle|DeliveryPersonAge|DeliveryPersonRatings|TypeOfVehicle|DeliveryPersonID|Restaurant_latitude|Restaurant_longitude|Delivery_location_latitude|Delivery_location_longitude|Order_Date|Time_Orderd|Weather_idx|Road_traffic_density_idx|Type_of_order_idx|multiple_deliveries_idx|Time_orderd|IntervalPickup|\n",
      "+------+------------------+---------------+-----------------+---------------------+-------------+----------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-----------+------------------------+-----------------+-----------------------+-----------+--------------+\n",
      "|0x3474|    BANGRES15DEL01|     motorcycle|             28.0|                  4.6|          0.0|           206.0|          12.975377|           77.696664|                 13.085377|                  77.806664|29-03-2022|      20:30|        1.0|                     1.0|              2.0|                    0.0|      20:30|           300|\n",
      "|0x9420|     JAPRES09DEL03|     motorcycle|             23.0|                  4.5|          0.0|           464.0|          26.911378|           75.789034|                 27.001378|                  75.879034|10-03-2022|      19:35|        4.0|                     1.0|              0.0|                    0.0|      19:35|           600|\n",
      "+------+------------------+---------------+-----------------+---------------------+-------------+----------------+-------------------+--------------------+--------------------------+---------------------------+----------+-----------+-----------+------------------------+-----------------+-----------------------+-----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assemblingFinal = assemblingData_2.drop(\"pick_ID\")\n",
    "assembled = assemblingFinal.drop(\"Time_Order_picked\")\n",
    "assembled.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "3f799f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled.drop(\"Time_orderd\").write. \\\n",
    "    csv(externalData+\"amazonRA_encoded.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3185c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresDS = assemblingFinal.select(\"DeliveryPersonID\",\"TypeOfVehicle\",\n",
    "                                   \"DeliveryPersonAge\",\"DeliveryPersonRatings\",\n",
    "                                   \"Restaurant_latitude\",\"Restaurant_longitude\",\n",
    "                                   \"Delivery_location_latitude\",\"Delivery_location_longitude\",\n",
    "                                   \"Weather_idx\",\"Type_of_order_idx\",\"multiple_deliveries_idx\",\n",
    "                                   \"IntervalPickup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "75ad9907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writingCSVFiletoDatabase(session, csvFile,dbName,dbTableName):\n",
    "    \n",
    "    fileSparkDF = session.read.csv(csvFile,inferSchema=True,header=True)\n",
    "    try:\n",
    "        fileSparkDF.write \\\n",
    "                    .format('jdbc') \\\n",
    "                    .option(\"url\", f\"jdbc:postgresql://localhost:5432/{dbName}\") \\\n",
    "                    .option('dbtable', dbTableName) \\\n",
    "                    .option('user','postgres') \\\n",
    "                    .option('password', 1234) \\\n",
    "                    .option('driver','org.postgresql.Driver') \\\n",
    "                    .save(mode='overwrite')\n",
    "        print('Write Complete')\n",
    "    except Exception as e:\n",
    "        print(f'Write errored out due to {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4424088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writingSparkDFtoDatabase(session,sparkDF,dbName,dbTableName):\n",
    "    \n",
    "    try:\n",
    "        sparkDF.write \\\n",
    "                    .format('jdbc') \\\n",
    "                    .option(\"url\", f\"jdbc:postgresql://localhost:5432/{dbName}\") \\\n",
    "                    .option('dbtable', dbTableName) \\\n",
    "                    .option('user','postgres') \\\n",
    "                    .option('password', 1234) \\\n",
    "                    .option('driver','org.postgresql.Driver') \\\n",
    "                    .save(mode='overwrite')\n",
    "        print('Write Complete')\n",
    "    except Exception as e:\n",
    "        print(f'Write errored out due to {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c6eddfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 200:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "writingSparkDFtoDatabase(sparkSQL,featuresDS,\"amazon_ra\",\"featuresDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "eaba9550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets try feature hasher first\n",
    "\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "featureHashmodel = FeatureHasher(inputCols=[\"DeliveryPersonID\",\"TypeOfVehicle\",\n",
    "                                   \"DeliveryPersonAge\",\"DeliveryPersonRatings\",\n",
    "                                   \"Restaurant_latitude\",\"Restaurant_longitude\",\n",
    "                                   \"Delivery_location_latitude\",\"Delivery_location_longitude\",\n",
    "                                   \"Weather_idx\",\"Type_of_order_idx\",\n",
    "                                    \"multiple_deliveries_idx\"],\n",
    "                                outputCol=\"finalFeatures\")\n",
    "\n",
    "featureHashed = featureHashmodel.transform(featuresDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "52db7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurecollected =featureHashed.select(\"IntervalPickup\", \"finalFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0824cbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|IntervalPickup|finalFeatures                                                                                                                                                |\n",
      "+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|300           |(262144,[19519,34661,53716,148639,149782,152942,153164,162757,171227,173694,213070],[1.0,206.0,2.0,77.806664,28.0,77.696664,12.975377,13.085377,0.0,0.0,4.6])|\n",
      "|600           |(262144,[19519,34661,53716,148639,149782,152942,153164,162757,171227,173694,213070],[4.0,464.0,0.0,75.879034,23.0,75.789034,26.911378,27.001378,0.0,0.0,4.5])|\n",
      "+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurecollected.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a2fdcd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets play a bit scalers\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler,RobustScaler\n",
    "from pyspark.ml.feature import MinMaxScaler, MaxAbsScaler, Bucketizer\n",
    "from pyspark.ml.feature import ElementwiseProduct, VectorAssembler, VectorSizeHint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8b5310fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+\n",
      "|IntervalPickup|       finalFeatures|      NormedFeatures|\n",
      "+--------------+--------------------+--------------------+\n",
      "|           300|(262144,[19519,34...|(262144,[19519,34...|\n",
      "|           600|(262144,[19519,34...|(262144,[19519,34...|\n",
      "|           300|(262144,[19519,34...|(262144,[19519,34...|\n",
      "|           900|(262144,[19519,34...|(262144,[19519,34...|\n",
      "|           600|(262144,[19519,34...|(262144,[19519,34...|\n",
      "+--------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normed = Normalizer(inputCol='finalFeatures',outputCol=\"NormedFeatures\")\n",
    "\n",
    "featureNormed = normed.transform(featurecollected)\n",
    "featureNormed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "88e2458a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 10:22:44 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "+--------------+--------------------+--------------------+\n",
      "|IntervalPickup|       finalFeatures|      NormedFeatures|\n",
      "+--------------+--------------------+--------------------+\n",
      "|           300|(262144,[19519,34...|[0.0,0.0,0.0,0.0,...|\n",
      "|           600|(262144,[19519,34...|[0.0,0.0,0.0,0.0,...|\n",
      "|           300|(262144,[19519,34...|[0.0,0.0,0.0,0.0,...|\n",
      "|           900|(262144,[19519,34...|[0.0,0.0,0.0,0.0,...|\n",
      "|           600|(262144,[19519,34...|[0.0,0.0,0.0,0.0,...|\n",
      "+--------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "minMaxed = StandardScaler(withMean=True,inputCol='finalFeatures',outputCol=\"NormedFeatures\")\n",
    "minFit = minMaxed.fit(featurecollected)\n",
    "featurMinMaxed = minFit.transform(featurecollected)\n",
    "featurMinMaxed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9989645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------------------------------------------------------------+\n",
      "|intervalPickup|features                                                                |\n",
      "+--------------+------------------------------------------------------------------------+\n",
      "|300           |[206.0,0.0,28.0,4.6,12.975377,77.696664,13.085377,77.806664,1.0,2.0,0.0]|\n",
      "|600           |[464.0,0.0,23.0,4.5,26.911378,75.789034,27.001378,75.879034,4.0,0.0,0.0]|\n",
      "+--------------+------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#simple vector assembler\n",
    "vectAssembled = VectorAssembler(inputCols=[\"DeliveryPersonID\",\"TypeOfVehicle\",\n",
    "                                   \"DeliveryPersonAge\",\"DeliveryPersonRatings\",\n",
    "                                   \"Restaurant_latitude\",\"Restaurant_longitude\",\n",
    "                                   \"Delivery_location_latitude\",\"Delivery_location_longitude\",\n",
    "                                   \"Weather_idx\",\"Type_of_order_idx\",\"multiple_deliveries_idx\"],\n",
    "                               outputCol=\"features\")\n",
    "assembledVectors = vectAssembled.transform(featuresDS).select(\"intervalPickup\",\n",
    "                                                             \"features\")\n",
    "assembledVectors.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "86cdc2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next comes the feature selectors\n",
    "from pyspark.ml.feature import VarianceThresholdSelector, BucketedRandomProjectionLSH\n",
    "from pyspark.ml.feature import MinHashLSH, UnivariateFeatureSelector, ChiSqSelector\n",
    "from pyspark.ml.feature import VectorSlicer, RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8b55b035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 10:37:55 ERROR Executor: Exception in task 0.0 in stage 234.0 (TID 218)\n",
      "scala.MatchError: [null,[282.0,1.0,23.0,4.8,12.914264,77.6784,12.934264,77.6984,1.0,1.0,1.0]] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.stat.ChiSquareTest$.$anonfun$test$1(ChiSquareTest.scala:82)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/11/29 10:37:55 WARN TaskSetManager: Lost task 0.0 in stage 234.0 (TID 218) (172.17.0.1 executor driver): scala.MatchError: [null,[282.0,1.0,23.0,4.8,12.914264,77.6784,12.934264,77.6984,1.0,1.0,1.0]] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.stat.ChiSquareTest$.$anonfun$test$1(ChiSquareTest.scala:82)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/11/29 10:37:55 ERROR TaskSetManager: Task 0 in stage 234.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1858.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 234.0 failed 1 times, most recent failure: Lost task 0.0 in stage 234.0 (TID 218) (172.17.0.1 executor driver): scala.MatchError: [null,[282.0,1.0,23.0,4.8,12.914264,77.6784,12.934264,77.6984,1.0,1.0,1.0]] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.stat.ChiSquareTest$.$anonfun$test$1(ChiSquareTest.scala:82)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1111)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1093)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1545)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1533)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:204)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3120)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3120)\n\tat org.apache.spark.ml.feature.Selector.getTopIndices$1(Selector.scala:216)\n\tat org.apache.spark.ml.feature.Selector.fit(Selector.scala:222)\n\tat org.apache.spark.ml.feature.ChiSqSelector.fit(ChiSqSelector.scala:111)\n\tat org.apache.spark.ml.feature.ChiSqSelector.fit(ChiSqSelector.scala:49)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: scala.MatchError: [null,[282.0,1.0,23.0,4.8,12.914264,77.6784,12.934264,77.6984,1.0,1.0,1.0]] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.stat.ChiSquareTest$.$anonfun$test$1(ChiSquareTest.scala:82)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [189], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m selector \u001b[38;5;241m=\u001b[39m ChiSqSelector(numTopFeatures\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                          outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselectedFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m                          labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintervalPickup\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43massembledVectors\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(assembledVectors)\n\u001b[1;32m      6\u001b[0m result\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1858.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 234.0 failed 1 times, most recent failure: Lost task 0.0 in stage 234.0 (TID 218) (172.17.0.1 executor driver): scala.MatchError: [null,[282.0,1.0,23.0,4.8,12.914264,77.6784,12.934264,77.6984,1.0,1.0,1.0]] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.stat.ChiSquareTest$.$anonfun$test$1(ChiSquareTest.scala:82)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1111)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1093)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1545)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1533)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:204)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3120)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3120)\n\tat org.apache.spark.ml.feature.Selector.getTopIndices$1(Selector.scala:216)\n\tat org.apache.spark.ml.feature.Selector.fit(Selector.scala:222)\n\tat org.apache.spark.ml.feature.ChiSqSelector.fit(ChiSqSelector.scala:111)\n\tat org.apache.spark.ml.feature.ChiSqSelector.fit(ChiSqSelector.scala:49)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: scala.MatchError: [null,[282.0,1.0,23.0,4.8,12.914264,77.6784,12.934264,77.6984,1.0,1.0,1.0]] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.stat.ChiSquareTest$.$anonfun$test$1(ChiSquareTest.scala:82)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", \n",
    "                         labelCol=\"intervalPickup\")\n",
    "\n",
    "result = selector.fit(assembledVectors).transform(assembledVectors)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "05515505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 10:40:21 ERROR Executor: Exception in task 0.0 in stage 243.0 (TID 225)\n",
      "java.lang.NullPointerException: Null value appeared in non-nullable field:\n",
      "- field (class: \"scala.Double\", name: \"_1\")\n",
      "- root class: \"scala.Tuple2\"\n",
      "If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.NewInstance_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.ml.stat.FValueTest$.$anonfun$testRegression$1(FValueTest.scala:128)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/11/29 10:40:21 WARN TaskSetManager: Lost task 0.0 in stage 243.0 (TID 225) (172.17.0.1 executor driver): java.lang.NullPointerException: Null value appeared in non-nullable field:\n",
      "- field (class: \"scala.Double\", name: \"_1\")\n",
      "- root class: \"scala.Tuple2\"\n",
      "If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.NewInstance_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.ml.stat.FValueTest$.$anonfun$testRegression$1(FValueTest.scala:128)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/11/29 10:40:21 ERROR TaskSetManager: Task 0 in stage 243.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1894.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 243.0 failed 1 times, most recent failure: Lost task 0.0 in stage 243.0 (TID 225) (172.17.0.1 executor driver): java.lang.NullPointerException: Null value appeared in non-nullable field:\n- field (class: \"scala.Double\", name: \"_1\")\n- root class: \"scala.Tuple2\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.NewInstance_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.ml.stat.FValueTest$.$anonfun$testRegression$1(FValueTest.scala:128)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1111)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1093)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1545)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1533)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:204)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3120)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3120)\n\tat org.apache.spark.ml.feature.UnivariateFeatureSelector.getTopIndices(UnivariateFeatureSelector.scala:203)\n\tat org.apache.spark.ml.feature.UnivariateFeatureSelector.selectIndicesFromPValues(UnivariateFeatureSelector.scala:218)\n\tat org.apache.spark.ml.feature.UnivariateFeatureSelector.fit(UnivariateFeatureSelector.scala:192)\n\tat org.apache.spark.ml.feature.UnivariateFeatureSelector.fit(UnivariateFeatureSelector.scala:129)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- field (class: \"scala.Double\", name: \"_1\")\n- root class: \"scala.Tuple2\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.NewInstance_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.ml.stat.FValueTest$.$anonfun$testRegression$1(FValueTest.scala:128)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [190], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m selector \u001b[38;5;241m=\u001b[39m UnivariateFeatureSelector(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                          outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselectedFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m                          labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintervalPickup\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m                         selectionMode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercentile\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m selector\u001b[38;5;241m.\u001b[39msetFeatureType(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39m \\\n\u001b[1;32m      6\u001b[0m             setLabelType(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39m \\\n\u001b[1;32m      7\u001b[0m             setSelectionThreshold(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43massembledVectors\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(assembledVectors)\n\u001b[1;32m      9\u001b[0m result\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1894.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 243.0 failed 1 times, most recent failure: Lost task 0.0 in stage 243.0 (TID 225) (172.17.0.1 executor driver): java.lang.NullPointerException: Null value appeared in non-nullable field:\n- field (class: \"scala.Double\", name: \"_1\")\n- root class: \"scala.Tuple2\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.NewInstance_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.ml.stat.FValueTest$.$anonfun$testRegression$1(FValueTest.scala:128)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1111)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1093)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1545)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1533)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:204)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3120)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3120)\n\tat org.apache.spark.ml.feature.UnivariateFeatureSelector.getTopIndices(UnivariateFeatureSelector.scala:203)\n\tat org.apache.spark.ml.feature.UnivariateFeatureSelector.selectIndicesFromPValues(UnivariateFeatureSelector.scala:218)\n\tat org.apache.spark.ml.feature.UnivariateFeatureSelector.fit(UnivariateFeatureSelector.scala:192)\n\tat org.apache.spark.ml.feature.UnivariateFeatureSelector.fit(UnivariateFeatureSelector.scala:129)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException: Null value appeared in non-nullable field:\n- field (class: \"scala.Double\", name: \"_1\")\n- root class: \"scala.Tuple2\"\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.NewInstance_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.ml.stat.FValueTest$.$anonfun$testRegression$1(FValueTest.scala:128)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "selector = UnivariateFeatureSelector(featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", \n",
    "                         labelCol=\"intervalPickup\",\n",
    "                        selectionMode='percentile')\n",
    "selector.setFeatureType(\"continuous\"). \\\n",
    "            setLabelType(\"continuous\"). \\\n",
    "            setSelectionThreshold(1)\n",
    "result = selector.fit(assembledVectors).transform(assembledVectors)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "89bd8a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------------------------------------------------------------+----------------------------------------------------+\n",
      "|intervalPickup|features                                                                |selectedFeatures                                    |\n",
      "+--------------+------------------------------------------------------------------------+----------------------------------------------------+\n",
      "|300           |[206.0,0.0,28.0,4.6,12.975377,77.696664,13.085377,77.806664,1.0,2.0,0.0]|[206.0,28.0,12.975377,77.696664,13.085377,77.806664]|\n",
      "|600           |[464.0,0.0,23.0,4.5,26.911378,75.789034,27.001378,75.879034,4.0,0.0,0.0]|[464.0,23.0,26.911378,75.789034,27.001378,75.879034]|\n",
      "+--------------+------------------------------------------------------------------------+----------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selector = VarianceThresholdSelector(featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\",\n",
    "                        varianceThreshold=10)\n",
    "\n",
    "result = selector.fit(assembledVectors).transform(assembledVectors)\n",
    "result.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "70a0add7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  300|[206.0,28.0,12.97...|\n",
      "|  600|[464.0,23.0,26.91...|\n",
      "+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = result.select(col(\"intervalPickup\").alias('label'),\n",
    "                     col('selectedFeatures').alias('features'))\n",
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "808d9aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  300|[206.0,0.0,28.0,4...|\n",
      "|  600|[464.0,0.0,23.0,4...|\n",
      "+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_rev1 = result.select(col(\"intervalPickup\").alias('label'),\n",
    "                     col('features'))\n",
    "data_rev1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "58b5a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, LinearRegression, GeneralizedLinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor,GBTRegressor, DecisionTreeRegressionModel\n",
    "from pyspark.ml.regression import FMRegressor,AFTSurvivalRegression,IsotonicRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0f345d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10668"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c14c6743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7445"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There are multiple algorithms, so testing in couple of options\n",
    "(train,test) = data.randomSplit([0.7,0.3])\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0010a0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2045"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select(\"label\").filter(col(\"label\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b382ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = data.dropna('any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "6dec123c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6081"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There are multiple algorithms, so testing in couple of options\n",
    "(train_r,test_r) = newData.randomSplit([0.7,0.3])\n",
    "train_r.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "559b0c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 383:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 11:10:28 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/11/29 11:10:28 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 386:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a5c45917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0036566411613385284,0.01879865357540184,0.13732133983113926,0.0,0.32296805912230897,0.0]\n",
      "Intercept: 588.9996549691938\n",
      "numIterations: 10\n",
      "objectiveHistory: [0.5000000000000009, 0.4999257507605972, 0.4999025194010279, 0.4998968668985165, 0.4998941239929033, 0.4998938869481651, 0.4998938429985969, 0.4998937858020532, 0.49989373978310614, 0.499893639618846, 0.49989362870555504]\n",
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|  -289.910760976033|\n",
      "|  -289.804992370312|\n",
      "| -289.8102928715506|\n",
      "|-289.81998191332434|\n",
      "|-290.56468033352235|\n",
      "| -291.1415027040979|\n",
      "| -291.1915762976206|\n",
      "|-292.12386732436016|\n",
      "|-292.68936582265724|\n",
      "| -292.7871682010974|\n",
      "|-292.93296690656257|\n",
      "|-293.28891797913775|\n",
      "| -293.4363755168928|\n",
      "|-293.62198629319573|\n",
      "|-293.49268299042035|\n",
      "|-293.66875719435154|\n",
      "| -293.7000566392942|\n",
      "| -294.4689209899525|\n",
      "| -294.5247375207067|\n",
      "| -294.6193102185557|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 244.614536\n",
      "r2: 0.000250\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4693c24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----------------+\n",
      "|label|            features|       prediction|\n",
      "+-----+--------------------+-----------------+\n",
      "|  300|(6,[0,1,4,5],[84....|589.8933801981468|\n",
      "|  300|(6,[0,1,4,5],[116...|590.0485694524324|\n",
      "|  300|(6,[0,1,4,5],[433...|591.2855695654977|\n",
      "|  300|(6,[0,1,4,5],[599...|591.9398583472043|\n",
      "|  300|(6,[0,1,4,5],[643...|591.9154142731685|\n",
      "|  300|(6,[0,1,4,5],[731...|592.2598064595048|\n",
      "|  300|(6,[0,1,4,5],[746...|592.4209886371948|\n",
      "|  300|(6,[0,1,4,5],[753...|592.4868326830943|\n",
      "|  300|(6,[0,1,4,5],[789...|592.3015449047399|\n",
      "|  300|(6,[0,1,4,5],[841...|592.5427857046171|\n",
      "|  300|(6,[0,1,4,5],[856...|592.7345263983036|\n",
      "|  300|(6,[0,1,4,5],[106...|593.3690341376779|\n",
      "|  300|(6,[0,1,4,5],[106...|593.4727164473286|\n",
      "|  300|(6,[0,1,4,5],[113...|593.6155509337983|\n",
      "|  300|[0.0,31.0,11.0162...|594.6660171375082|\n",
      "|  300|[1.0,26.0,18.9942...| 598.251070938581|\n",
      "|  300|[1.0,35.0,18.9942...|598.4299478625334|\n",
      "|  300|[2.0,29.0,11.0036...|594.6525278819993|\n",
      "|  300|[2.0,31.0,11.0036...|594.6772064667852|\n",
      "|  300|[3.0,38.0,22.7615...|600.2374201762395|\n",
      "+-----+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrModel.transform(test_r).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b4948148",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmregressor = FMRegressor(featuresCol='features',\n",
    "    labelCol= 'label',\n",
    "    predictionCol= 'prediction',\n",
    "    factorSize = 8,\n",
    "    fitIntercept = True,\n",
    "    fitLinear = True,\n",
    "    regParam = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "1bb80cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmModel = fmregressor.fit(train_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "f826b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionFM = fmModel.transform(test_r).select(\"label\",\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b77ce696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|label|        prediction|\n",
      "+-----+------------------+\n",
      "|  300|3.8420694336554657|\n",
      "|  300| 4.824404241603713|\n",
      "+-----+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionFM.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "61dbb278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 946:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Complete\n"
     ]
    }
   ],
   "source": [
    "writingSparkDFtoDatabase(sparkSQL,predictionFM,'amazon_ra','predictionFM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
