{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa85bc5",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "Understand the recommender system and the FP mining models in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4c9db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting with import of pyspark and related modules\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d2573f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 15:10:33 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 192.168.224.83 instead (on interface wlo1)\n",
      "22/11/29 15:10:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/11/29 15:10:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#Initiating the spark session with postgres driver    \n",
    "sparkSQL = SparkSession.builder.appName('Spark SQL') \\\n",
    "        .config('spark.jars',\"/usr/share/java/postgresql-42.2.26.jar\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39382b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "mllibPath = \"mllib/\"\n",
    "externalData = \"externalData/\"\n",
    "ytDE = \"/home/solverbot/Desktop/ytDE/csvfiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb1c11d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkread = sparkSQL.read\n",
    "sparkcont = sparkSQL.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cebdfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#readin the movielens data\n",
    "\n",
    "movielens = sparkcont.textFile(mllibPath+\"sample_movielens_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6231673e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0::2::3\n",
      "0::3::1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for i in movielens.take(2): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbe12dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "movieTable = movielens.map(lambda x: [int(u) for u in x.split(\"::\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dcf1468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3]\n",
      "[0, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in movieTable.take(2): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d7a644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "moviedf = sparkSQL.createDataFrame(movieTable,[\"user_id\",\"movie_id\",\"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84556cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "moviedf.write.csv(mllibPath+\"movielens.csv\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6f7dae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|user_id|movie_id|rating|\n",
      "+-------+--------+------+\n",
      "|      0|       2|     3|\n",
      "|      0|       3|     1|\n",
      "+-------+--------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviedf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7cd62f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|           user_id|          movie_id|            rating|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|              1501|              1501|              1501|\n",
      "|   mean|14.383744170552964| 49.40572951365756|1.7741505662891406|\n",
      "| stddev| 8.591040424293267|28.937034065089016|1.1872761661248032|\n",
      "|    min|                 0|                 0|                 1|\n",
      "|    max|                29|                99|                 5|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviedf.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a41fbb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = moviedf.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fefe0612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ALS in module pyspark.ml.recommendation:\n",
      "\n",
      "class ALS(pyspark.ml.wrapper.JavaEstimator, _ALSParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n",
      " |  ALS(*, rank: int = 10, maxIter: int = 10, regParam: float = 0.1, numUserBlocks: int = 10, numItemBlocks: int = 10, implicitPrefs: bool = False, alpha: float = 1.0, userCol: str = 'user', itemCol: str = 'item', seed: Optional[int] = None, ratingCol: str = 'rating', nonnegative: bool = False, checkpointInterval: int = 10, intermediateStorageLevel: str = 'MEMORY_AND_DISK', finalStorageLevel: str = 'MEMORY_AND_DISK', coldStartStrategy: str = 'nan', blockSize: int = 4096)\n",
      " |  \n",
      " |  Alternating Least Squares (ALS) matrix factorization.\n",
      " |  \n",
      " |  ALS attempts to estimate the ratings matrix `R` as the product of\n",
      " |  two lower-rank matrices, `X` and `Y`, i.e. `X * Yt = R`. Typically\n",
      " |  these approximations are called 'factor' matrices. The general\n",
      " |  approach is iterative. During each iteration, one of the factor\n",
      " |  matrices is held constant, while the other is solved for using least\n",
      " |  squares. The newly-solved factor matrix is then held constant while\n",
      " |  solving for the other factor matrix.\n",
      " |  \n",
      " |  This is a blocked implementation of the ALS factorization algorithm\n",
      " |  that groups the two sets of factors (referred to as \"users\" and\n",
      " |  \"products\") into blocks and reduces communication by only sending\n",
      " |  one copy of each user vector to each product block on each\n",
      " |  iteration, and only for the product blocks that need that user's\n",
      " |  feature vector. This is achieved by pre-computing some information\n",
      " |  about the ratings matrix to determine the \"out-links\" of each user\n",
      " |  (which blocks of products it will contribute to) and \"in-link\"\n",
      " |  information for each product (which of the feature vectors it\n",
      " |  receives from each user block it will depend on). This allows us to\n",
      " |  send only an array of feature vectors between each user block and\n",
      " |  product block, and have the product block find the users' ratings\n",
      " |  and update the products based on these messages.\n",
      " |  \n",
      " |  For implicit preference data, the algorithm used is based on\n",
      " |  `\"Collaborative Filtering for Implicit Feedback Datasets\",\n",
      " |  <https://doi.org/10.1109/ICDM.2008.22>`_, adapted for the blocked\n",
      " |  approach used here.\n",
      " |  \n",
      " |  Essentially instead of finding the low-rank approximations to the\n",
      " |  rating matrix `R`, this finds the approximations for a preference\n",
      " |  matrix `P` where the elements of `P` are 1 if r > 0 and 0 if r <= 0.\n",
      " |  The ratings then act as 'confidence' values related to strength of\n",
      " |  indicated user preferences rather than explicit ratings given to\n",
      " |  items.\n",
      " |  \n",
      " |  .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The input rating dataframe to the ALS implementation should be deterministic.\n",
      " |  Nondeterministic data can cause failure during fitting ALS model.\n",
      " |  For example, an order-sensitive operation like sampling after a repartition makes\n",
      " |  dataframe output nondeterministic, like `df.repartition(2).sample(False, 0.5, 1618)`.\n",
      " |  Checkpointing sampled dataframe or adding a sort before sampling can help make the\n",
      " |  dataframe deterministic.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> df = spark.createDataFrame(\n",
      " |  ...     [(0, 0, 4.0), (0, 1, 2.0), (1, 1, 3.0), (1, 2, 4.0), (2, 1, 1.0), (2, 2, 5.0)],\n",
      " |  ...     [\"user\", \"item\", \"rating\"])\n",
      " |  >>> als = ALS(rank=10, seed=0)\n",
      " |  >>> als.setMaxIter(5)\n",
      " |  ALS...\n",
      " |  >>> als.getMaxIter()\n",
      " |  5\n",
      " |  >>> als.setRegParam(0.1)\n",
      " |  ALS...\n",
      " |  >>> als.getRegParam()\n",
      " |  0.1\n",
      " |  >>> als.clear(als.regParam)\n",
      " |  >>> model = als.fit(df)\n",
      " |  >>> model.getBlockSize()\n",
      " |  4096\n",
      " |  >>> model.getUserCol()\n",
      " |  'user'\n",
      " |  >>> model.setUserCol(\"user\")\n",
      " |  ALSModel...\n",
      " |  >>> model.getItemCol()\n",
      " |  'item'\n",
      " |  >>> model.setPredictionCol(\"newPrediction\")\n",
      " |  ALS...\n",
      " |  >>> model.rank\n",
      " |  10\n",
      " |  >>> model.userFactors.orderBy(\"id\").collect()\n",
      " |  [Row(id=0, features=[...]), Row(id=1, ...), Row(id=2, ...)]\n",
      " |  >>> test = spark.createDataFrame([(0, 2), (1, 0), (2, 0)], [\"user\", \"item\"])\n",
      " |  >>> predictions = sorted(model.transform(test).collect(), key=lambda r: r[0])\n",
      " |  >>> predictions[0]\n",
      " |  Row(user=0, item=2, newPrediction=0.6929...)\n",
      " |  >>> predictions[1]\n",
      " |  Row(user=1, item=0, newPrediction=3.47356...)\n",
      " |  >>> predictions[2]\n",
      " |  Row(user=2, item=0, newPrediction=-0.899198...)\n",
      " |  >>> user_recs = model.recommendForAllUsers(3)\n",
      " |  >>> user_recs.where(user_recs.user == 0)        .select(\"recommendations.item\", \"recommendations.rating\").collect()\n",
      " |  [Row(item=[0, 1, 2], rating=[3.910..., 1.997..., 0.692...])]\n",
      " |  >>> item_recs = model.recommendForAllItems(3)\n",
      " |  >>> item_recs.where(item_recs.item == 2)        .select(\"recommendations.user\", \"recommendations.rating\").collect()\n",
      " |  [Row(user=[2, 1, 0], rating=[4.892..., 3.991..., 0.692...])]\n",
      " |  >>> user_subset = df.where(df.user == 2)\n",
      " |  >>> user_subset_recs = model.recommendForUserSubset(user_subset, 3)\n",
      " |  >>> user_subset_recs.select(\"recommendations.item\", \"recommendations.rating\").first()\n",
      " |  Row(item=[2, 1, 0], rating=[4.892..., 1.076..., -0.899...])\n",
      " |  >>> item_subset = df.where(df.item == 0)\n",
      " |  >>> item_subset_recs = model.recommendForItemSubset(item_subset, 3)\n",
      " |  >>> item_subset_recs.select(\"recommendations.user\", \"recommendations.rating\").first()\n",
      " |  Row(user=[0, 1, 2], rating=[3.910..., 3.473..., -0.899...])\n",
      " |  >>> als_path = temp_path + \"/als\"\n",
      " |  >>> als.save(als_path)\n",
      " |  >>> als2 = ALS.load(als_path)\n",
      " |  >>> als.getMaxIter()\n",
      " |  5\n",
      " |  >>> model_path = temp_path + \"/als_model\"\n",
      " |  >>> model.save(model_path)\n",
      " |  >>> model2 = ALSModel.load(model_path)\n",
      " |  >>> model.rank == model2.rank\n",
      " |  True\n",
      " |  >>> sorted(model.userFactors.collect()) == sorted(model2.userFactors.collect())\n",
      " |  True\n",
      " |  >>> sorted(model.itemFactors.collect()) == sorted(model2.itemFactors.collect())\n",
      " |  True\n",
      " |  >>> model.transform(test).take(1) == model2.transform(test).take(1)\n",
      " |  True\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ALS\n",
      " |      pyspark.ml.wrapper.JavaEstimator\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Estimator\n",
      " |      _ALSParams\n",
      " |      _ALSModelParams\n",
      " |      pyspark.ml.param.shared.HasPredictionCol\n",
      " |      pyspark.ml.param.shared.HasBlockSize\n",
      " |      pyspark.ml.param.shared.HasMaxIter\n",
      " |      pyspark.ml.param.shared.HasRegParam\n",
      " |      pyspark.ml.param.shared.HasCheckpointInterval\n",
      " |      pyspark.ml.param.shared.HasSeed\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, rank: int = 10, maxIter: int = 10, regParam: float = 0.1, numUserBlocks: int = 10, numItemBlocks: int = 10, implicitPrefs: bool = False, alpha: float = 1.0, userCol: str = 'user', itemCol: str = 'item', seed: Optional[int] = None, ratingCol: str = 'rating', nonnegative: bool = False, checkpointInterval: int = 10, intermediateStorageLevel: str = 'MEMORY_AND_DISK', finalStorageLevel: str = 'MEMORY_AND_DISK', coldStartStrategy: str = 'nan', blockSize: int = 4096)\n",
      " |      __init__(self, \\*, rank=10, maxIter=10, regParam=0.1, numUserBlocks=10,\n",
      " |               numItemBlocks=10, implicitPrefs=False, alpha=1.0, userCol=\"user\", itemCol=\"item\",                  seed=None, ratingCol=\"rating\", nonnegative=False, checkpointInterval=10,                  intermediateStorageLevel=\"MEMORY_AND_DISK\",                  finalStorageLevel=\"MEMORY_AND_DISK\", coldStartStrategy=\"nan\", blockSize=4096)\n",
      " |  \n",
      " |  setAlpha(self, value: float) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`alpha`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setBlockSize(self, value: int) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`blockSize`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setCheckpointInterval(self, value: int) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`checkpointInterval`.\n",
      " |  \n",
      " |  setColdStartStrategy(self, value: str) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`coldStartStrategy`.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |  \n",
      " |  setFinalStorageLevel(self, value: str) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`finalStorageLevel`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  setImplicitPrefs(self, value: bool) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`implicitPrefs`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setIntermediateStorageLevel(self, value: str) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`intermediateStorageLevel`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  setItemCol(self, value: str) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`itemCol`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setMaxIter(self, value: int) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`maxIter`.\n",
      " |  \n",
      " |  setNonnegative(self, value: bool) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`nonnegative`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setNumBlocks(self, value: int) -> 'ALS'\n",
      " |      Sets both :py:attr:`numUserBlocks` and :py:attr:`numItemBlocks` to the specific value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setNumItemBlocks(self, value: int) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`numItemBlocks`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setNumUserBlocks(self, value: int) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`numUserBlocks`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setParams(self, *, rank: int = 10, maxIter: int = 10, regParam: float = 0.1, numUserBlocks: int = 10, numItemBlocks: int = 10, implicitPrefs: bool = False, alpha: float = 1.0, userCol: str = 'user', itemCol: str = 'item', seed: Optional[int] = None, ratingCol: str = 'rating', nonnegative: bool = False, checkpointInterval: int = 10, intermediateStorageLevel: str = 'MEMORY_AND_DISK', finalStorageLevel: str = 'MEMORY_AND_DISK', coldStartStrategy: str = 'nan', blockSize: int = 4096) -> 'ALS'\n",
      " |      setParams(self, \\*, rank=10, maxIter=10, regParam=0.1, numUserBlocks=10,                  numItemBlocks=10, implicitPrefs=False, alpha=1.0, userCol=\"user\", itemCol=\"item\",                  seed=None, ratingCol=\"rating\", nonnegative=False, checkpointInterval=10,                  intermediateStorageLevel=\"MEMORY_AND_DISK\",                  finalStorageLevel=\"MEMORY_AND_DISK\", coldStartStrategy=\"nan\", blockSize=4096)\n",
      " |      Sets params for ALS.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setPredictionCol(self, value: str) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`predictionCol`.\n",
      " |  \n",
      " |  setRank(self, value: int) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`rank`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setRatingCol(self, value: str) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`ratingCol`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setRegParam(self, value: float) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`regParam`.\n",
      " |  \n",
      " |  setSeed(self, value: int) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`seed`.\n",
      " |  \n",
      " |  setUserCol(self, value: str) -> 'ALS'\n",
      " |      Sets the value of :py:attr:`userCol`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n",
      " |  \n",
      " |  __orig_bases__ = (pyspark.ml.wrapper.JavaEstimator[ForwardRef('ALSMode...\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  clear(self, param: pyspark.ml.param.Param) -> None\n",
      " |      Clears a param from the param map if it has been explicitly set.\n",
      " |  \n",
      " |  copy(self: 'JP', extra: Optional[ForwardRef('ParamMap')] = None) -> 'JP'\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          Extra parameters to copy to the new instance\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`JavaParams`\n",
      " |          Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Estimator:\n",
      " |  \n",
      " |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n",
      " |      Fits a model to the input dataset with optional parameters.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          input dataset.\n",
      " |      params : dict or list or tuple, optional\n",
      " |          an optional param map that overrides embedded params. If a list/tuple of\n",
      " |          param maps is given, this calls fit on each param map and returns a list of\n",
      " |          models.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n",
      " |          fitted model(s)\n",
      " |  \n",
      " |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n",
      " |      Fits a model to the input dataset for each param map in `paramMaps`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          input dataset.\n",
      " |      paramMaps : :py:class:`collections.abc.Sequence`\n",
      " |          A Sequence of param maps.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`_FitMultipleIterator`\n",
      " |          A thread safe iterable which contains one model for each param map. Each\n",
      " |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n",
      " |          using `paramMaps[index]`. `index` values may not be sequential.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _ALSParams:\n",
      " |  \n",
      " |  getAlpha(self) -> float\n",
      " |      Gets the value of alpha or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  getFinalStorageLevel(self) -> str\n",
      " |      Gets the value of finalStorageLevel or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  getImplicitPrefs(self) -> bool\n",
      " |      Gets the value of implicitPrefs or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  getIntermediateStorageLevel(self) -> str\n",
      " |      Gets the value of intermediateStorageLevel or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  getNonnegative(self) -> bool\n",
      " |      Gets the value of nonnegative or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  getNumItemBlocks(self) -> int\n",
      " |      Gets the value of numItemBlocks or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  getNumUserBlocks(self) -> int\n",
      " |      Gets the value of numUserBlocks or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  getRank(self) -> int\n",
      " |      Gets the value of rank or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  getRatingCol(self) -> str\n",
      " |      Gets the value of ratingCol or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _ALSParams:\n",
      " |  \n",
      " |  alpha = Param(parent='undefined', name='alpha', doc='alpha for implici...\n",
      " |  \n",
      " |  finalStorageLevel = Param(parent='undefined', name='finalStorageLevel'...\n",
      " |  \n",
      " |  implicitPrefs = Param(parent='undefined', name='implicitPrefs', doc='w...\n",
      " |  \n",
      " |  intermediateStorageLevel = Param(parent='undefined', name='intermediat...\n",
      " |  \n",
      " |  nonnegative = Param(parent='undefined', name='nonnegative', do...to us...\n",
      " |  \n",
      " |  numItemBlocks = Param(parent='undefined', name='numItemBlocks', doc='n...\n",
      " |  \n",
      " |  numUserBlocks = Param(parent='undefined', name='numUserBlocks', doc='n...\n",
      " |  \n",
      " |  rank = Param(parent='undefined', name='rank', doc='rank of the factori...\n",
      " |  \n",
      " |  ratingCol = Param(parent='undefined', name='ratingCol', doc='column na...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _ALSModelParams:\n",
      " |  \n",
      " |  getColdStartStrategy(self) -> str\n",
      " |      Gets the value of coldStartStrategy or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |  \n",
      " |  getItemCol(self) -> str\n",
      " |      Gets the value of itemCol or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  getUserCol(self) -> str\n",
      " |      Gets the value of userCol or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _ALSModelParams:\n",
      " |  \n",
      " |  coldStartStrategy = Param(parent='undefined', name='coldStartStrateg.....\n",
      " |  \n",
      " |  itemCol = Param(parent='undefined', name='itemCol', doc='c...ds. Ids m...\n",
      " |  \n",
      " |  userCol = Param(parent='undefined', name='userCol', doc='c...ds. Ids m...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  getPredictionCol(self) -> str\n",
      " |      Gets the value of predictionCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasBlockSize:\n",
      " |  \n",
      " |  getBlockSize(self) -> int\n",
      " |      Gets the value of blockSize or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasBlockSize:\n",
      " |  \n",
      " |  blockSize = Param(parent='undefined', name='blockSize', doc=...n then ...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n",
      " |  \n",
      " |  getMaxIter(self) -> int\n",
      " |      Gets the value of maxIter or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n",
      " |  \n",
      " |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasRegParam:\n",
      " |  \n",
      " |  getRegParam(self) -> float\n",
      " |      Gets the value of regParam or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasRegParam:\n",
      " |  \n",
      " |  regParam = Param(parent='undefined', name='regParam', doc='regularizat...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n",
      " |  \n",
      " |  getCheckpointInterval(self) -> int\n",
      " |      Gets the value of checkpointInterval or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n",
      " |  \n",
      " |  checkpointInterval = Param(parent='undefined', name='checkpointInterv....\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n",
      " |  \n",
      " |  getSeed(self) -> int\n",
      " |      Gets the value of seed or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n",
      " |  \n",
      " |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self) -> str\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra: Optional[ForwardRef('ParamMap')] = None) -> 'ParamMap'\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          extra param values\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName: str) -> bool\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self) -> pyspark.ml.util.JavaMLWriter\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path: str) -> None\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path: str) -> ~RL from abc.ABCMeta\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from abc.ABCMeta\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "help(ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e0b12c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=15, regParam=0.01, userCol='user_id', \n",
    "          itemCol='movie_id', ratingCol='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "443dfb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|rating|prediction|\n",
      "+------+----------+\n",
      "|     3|-0.8297616|\n",
      "|     1| 0.8943969|\n",
      "+------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = als.fit(train)\n",
    "prediction = model.transform(test)\n",
    "prediction.select(\"rating\",\"prediction\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f78e1e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.6637695440319358\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName = 'rmse', labelCol = 'rating', predictionCol = 'prediction')\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9379d295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|user_id|movie_id|\n",
      "+-------+--------+\n",
      "|     12|       2|\n",
      "|     12|       7|\n",
      "|     12|       8|\n",
      "|     12|      14|\n",
      "|     12|      22|\n",
      "|     12|      23|\n",
      "|     12|      24|\n",
      "|     12|      25|\n",
      "|     12|      30|\n",
      "|     12|      31|\n",
      "|     12|      41|\n",
      "|     12|      53|\n",
      "|     12|      60|\n",
      "|     12|      63|\n",
      "|     12|      72|\n",
      "|     12|      77|\n",
      "|     12|      91|\n",
      "|     12|      95|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "this_user = test.filter(test['user_id'] == 12).select('user_id', 'movie_id')\n",
    "this_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5819a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+\n",
      "|user_id|movie_id| prediction|\n",
      "+-------+--------+-----------+\n",
      "|     12|      31|  1.6109191|\n",
      "|     12|      53|  1.9744065|\n",
      "|     12|      91|   3.446911|\n",
      "|     12|      22|  0.7696948|\n",
      "|     12|      41|  1.9108294|\n",
      "|     12|      72|  4.3716397|\n",
      "|     12|       8| -2.3297799|\n",
      "|     12|      23|  4.7609572|\n",
      "|     12|       7|  2.0911324|\n",
      "|     12|      63|  1.5779959|\n",
      "|     12|      77|-0.29248747|\n",
      "|     12|      25|  3.4542992|\n",
      "|     12|      24| 0.73902446|\n",
      "|     12|      95| 0.16467327|\n",
      "|     12|      60|  1.5980449|\n",
      "|     12|      14|  2.5773976|\n",
      "|     12|       2| -1.9526407|\n",
      "|     12|      30|  3.4185607|\n",
      "+-------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendation_this_user = model.transform(this_user)\n",
    "recommendation_this_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49ea37f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+\n",
      "|user_id|movie_id| prediction|\n",
      "+-------+--------+-----------+\n",
      "|     12|      23|  4.7609572|\n",
      "|     12|      72|  4.3716397|\n",
      "|     12|      25|  3.4542992|\n",
      "|     12|      91|   3.446911|\n",
      "|     12|      30|  3.4185607|\n",
      "|     12|      14|  2.5773976|\n",
      "|     12|       7|  2.0911324|\n",
      "|     12|      53|  1.9744065|\n",
      "|     12|      41|  1.9108294|\n",
      "|     12|      31|  1.6109191|\n",
      "|     12|      60|  1.5980449|\n",
      "|     12|      63|  1.5779959|\n",
      "|     12|      22|  0.7696948|\n",
      "|     12|      24| 0.73902446|\n",
      "|     12|      95| 0.16467327|\n",
      "|     12|      77|-0.29248747|\n",
      "|     12|       2| -1.9526407|\n",
      "|     12|       8| -2.3297799|\n",
      "+-------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendation_this_user.orderBy(\"prediction\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f2d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285dd9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
