{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a1cf49d",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "Notebook consists of multiple datasets loaded from the sources like \n",
    "\n",
    "- mllib examples\n",
    "\n",
    "- Kaggle datasets\n",
    "\n",
    "- Data from Observable HQ\n",
    "\n",
    "- Data scraped from websites\n",
    "\n",
    "These datasets will be reviewed to check the type of analysis, machine learning algorithms that can be done on it. The points will be listed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting with import of pyspark and related modules\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pyspark.ml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf6b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiating the spark session\n",
    "spark = SparkSession.builder.appName(\"Reviews\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mllibPath = \"mllib/\"\n",
    "externalData = \"externalData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6244fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkReader = spark.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7087ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d784af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gmm_data\n",
    "\n",
    "gmmData = sparkReader.text(mllibPath+'gmm_data.txt')\n",
    "gmmData.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c813ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gmm_data\n",
    "\n",
    "gmmData = sparkReader.csv(mllibPath+'gmm_data.txt',sep=' '). \\\n",
    "            toDF(\"sl_no\",\"val1\",\"val2\"). \\\n",
    "            drop('sl_no')\n",
    "gmmData.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad65e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmmDataConv= gmmData.select(col(\"val1\").astype(\"float\"),\n",
    "               col(\"val2\").astype(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7360aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmmData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data has been converted to floats\n",
    "gmmDataConv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8778d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmmDataConv.describe().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f114af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In SQL there is option to provide serial number. \n",
    "#Is there such option in pyspark.sql.function\n",
    "help(pyspark.sql.functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30275c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = (col(\"val1\")).alias(\"key\")\n",
    "value = (randn(2) + key * 10).alias(\"glue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d39d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(0,2000,1,1).select(key, value).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8983af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(gmmDataConv.withColumn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45191e5",
   "metadata": {},
   "source": [
    "### Review on gmm_data\n",
    "\n",
    "- Both the columns are series of numbers in String format\n",
    "\n",
    "##### Modeling idea\n",
    "\n",
    "- The Val1 and Val2 correlation can be checked\n",
    "\n",
    "- Regression modeling can be done with either column considered as feature\n",
    "\n",
    "- Scatter plots, Histogram can be plotted for both the columns to understand the data\n",
    "\n",
    "- kmeans clustering can be applied on the values, as their means, standard deviations are completely different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans.txt\n",
    "\n",
    "kmeansData = sparkReader.format(\"csv\"). \\\n",
    "            load(mllibPath+\"kmeans_data.txt\"). \\\n",
    "            toDF(\"val1\",\"val2\",\"val3\")\n",
    "kmeansData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a034ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageRankData = sparkReader.format('csv').load(mllibPath+\"pagerank_data.txt\")\n",
    "pageRankData.show()\n",
    "#Not much is there... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleIsotonic = sparkReader.format('libsvm'). \\\n",
    "            load(mllibPath+\"sample_isotonic_regression_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleIsotonic.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f7fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264056e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lda = MLUtils.loadLibSVMFile(sc,mllibPath+\"sample_lda_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3449630",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sample_lda.take(2): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25841eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleLda_reader = sparkReader.format(\"libsvm\"). \\\n",
    "            option(\"numFeatures\",11). \\\n",
    "            load(mllibPath+\"sample_lda_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The libsvm is a file saved after the vector assembler has \n",
    "#written the features\n",
    "sampleLda_reader.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f34291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The MLUtils is RDD based API. Examples for the Ensembles were using it \n",
    "sampleLibsvm = MLUtils.loadLibSVMFile(sc,mllibPath+\"sample_libsvm_data.txt\")\n",
    "sampleLibsvm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284733db",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train,test) = sampleLibsvm.randomSplit([0.7,0.3])\n",
    "train_collected = train.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b757f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9eebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This way of loading RDD based libSVM will not work with reader method\n",
    "sampleLibSvm_load = sparkReader.format(\"libsvm\"). \\\n",
    "                option(\"numFeatures\",657). \\\n",
    "                load(mllibPath+\"sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe044da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleLinReg = sparkReader.format(\"libsvm\").\\\n",
    "            option(\"numFeatures\",10). \\\n",
    "            load(mllibPath+\"sample_linear_regression_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62fe50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleLinReg.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97207ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading movie lens data, fails. \n",
    "movielens = sparkReader.format('libsvm'). \\\n",
    "            option(\"numFeatures\",2). \\\n",
    "            load(mllibPath+\"sample_movielens_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading movie lens data, fails. \n",
    "movielens = sparkReader. \\\n",
    "            csv(mllibPath+\"sample_movielens_data.txt\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc76ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mcc = sparkReader.format('libsvm'). \\\n",
    "            option(\"numFeatures\",4). \\\n",
    "            load(mllibPath+\"sample_multiclass_classification_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c645d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mcc.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d552df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSvm = sparkReader.text(mllibPath+\"sample_svm_data.txt\")\n",
    "\n",
    "def splitRow(rows):\n",
    "    return [float(x) for x in rows.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sampleSvm.take(5):\n",
    "    print(splitRow(i.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSvm.select(col(\"value\").substr(0,1).alias('lables'),\n",
    "                col(\"value\").substr(3,10).alias('feature1')). \\\n",
    "        show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5292861",
   "metadata": {},
   "source": [
    "Need to figure out how to use the text files that are in the form of RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleText = sparkContext.textFile(mllibPath+\"sample_svm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84037218",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleText.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3eb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleTextTrns = sampleText.map(lambda x : [float(y) for y in x.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0942f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampleTextTrns.take(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601cee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sampleTextTrns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleTxtDF = sampleTextTrns.toDF([\"label\",\"feat2\",\"feat3\",\"feat4\",\"feat5\",\"feat6\",\n",
    "                     \"feat7\",\"feat8\",\"feat9\",\"feat10\",\"feat11\",\n",
    "                    \"feat12\",\"feat13\",\"feat14\",\"feat15\",\"feat16\",\n",
    "                                  \"feat17\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7058485",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sampleTxtDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16720665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.unpack_archive(externalData+\"amazon-business-research-analyst-dataset.zip\",extract_dir=externalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1d95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd externalData/\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "updatedAmazonRA = sparkReader.csv(externalData+\"updated.csv\",\n",
    "                                  inferSchema=True,\n",
    "                                  header=True,\n",
    "                                 sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6822fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedUpdatedAmazon = updatedAmazonRA.select(col('_c0').alias('slno'),\"*\").drop('_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4390c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedUpdatedAmazon.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92542715",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedUpdatedAmazon.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487d73bf",
   "metadata": {},
   "source": [
    "The Amazon dataset can be easily used for regression analysis to estimate the time taken by the delivery person\n",
    "\n",
    "The myriad of categorical columns are there, which can be used for data analysis and visualisations. A very informative dashboard can be implemented\n",
    "\n",
    "The same dataset can be used as the streaming dataset for creating dashboards that are live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ccf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_kleaned_RA = sparkReader.csv(externalData+\"encoded_cleaned_test.csv\",\n",
    "                                  inferSchema=True,\n",
    "                                  header=True,\n",
    "                                 sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00980f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kleaned_RA = sparkReader.csv(externalData+\"cleaned_test.csv\",\n",
    "                                  inferSchema=True,\n",
    "                                  header=True,\n",
    "                                 sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08df1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_kleaned_RA.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43464bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_kleaned_RA.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_kleaned_RA.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d2441",
   "metadata": {},
   "outputs": [],
   "source": [
    "kleaned_RA.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fd640",
   "metadata": {},
   "outputs": [],
   "source": [
    "kleaned_RA.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eeb9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kleaned_RA.select(\"ID\",\"Delivery_person_ID\",\"Delivery_person_Age\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets now concentrate on skillShare Courses dataset\n",
    "shutil.unpack_archive(externalData+\"skillshare-top-1000-course.zip\",\n",
    "                     extract_dir=externalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fdf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "skillshare = sparkReader.csv(externalData+\"data.csv\",\n",
    "                            inferSchema=True,\n",
    "                            header=True,\n",
    "                            sep=\",\")\n",
    "skillshare.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20846a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "skillshare.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a467b00",
   "metadata": {},
   "source": [
    "Skill share data can be used for estimating the number of students, based on the other features. KPI can be fixed based on the objective, and the number of students that needs to be reached.\n",
    "\n",
    "The data can be used for visualisation. Good dashboard can be generated, with bit of effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dbba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.unpack_archive(externalData+\"selected-indicators-from-world-bank-20002019.zip\",\n",
    "                        extract_dir=externalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334be122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are three tables in the indicators dataset\n",
    "countryDimension = sparkReader.csv(externalData+\"dimension_country.csv\",\n",
    "                                  sep=\",\",\n",
    "                                  inferSchema=True,\n",
    "                                  header=True)\n",
    "countryDimension.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f803038",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicatorDimension = sparkReader.csv(externalData+\"dimension_indicator.csv\",\n",
    "                                  sep=\",\",\n",
    "                                  inferSchema=True,\n",
    "                                  header=True)\n",
    "indicatorDimension.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80767c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "facttable = sparkReader.csv(externalData+\"facttable.csv\",\n",
    "                                  sep=\",\",\n",
    "                                  inferSchema=True,\n",
    "                                  header=True)\n",
    "facttable.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "facttable.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d511c2c7",
   "metadata": {},
   "source": [
    "With the indicators data available for the countries from the past 15 years, \n",
    "\n",
    "- time series analysis, \n",
    "\n",
    "- Regression can be done based on multiple indicators\n",
    "\n",
    "- Line chart, Choropleth charts can be developed\n",
    "\n",
    "The data can be sent to pocketbase portable database, and dashboards can be run from there, with help of AWS or Azure end points. There is no need for fancy databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f026827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets get some excel files into the system\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I am trying to import a multi-worksheet XL, \n",
    "#the \"Datasource\" sheet is required sheet in the xl\n",
    "\n",
    "datasource = pd.read_excel(\"sales Target Dashboard.xlsx\",sheet_name=\"DataSource\",\n",
    "                          parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77db0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = datasource.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd748f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedDS = datasource[['S/N', 'Date', 'Branch', 'Pizza Type', 'Quantity', 'Time', 'Time Range',\n",
    "       'Price', 'Daily Target','Unnamed: 14','Sales Target', 'Unnamed: 19',\n",
    "       'Branch.1', 'Unnamed: 24','Unnamed: 25']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15216e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sale Data is important table\n",
    "saleData = cleanedDS[['S/N', 'Date', 'Branch', \n",
    "                      'Pizza Type', 'Quantity', 'Time', \n",
    "                      'Time Range','Price']]\n",
    "saleData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intermediate table\n",
    "miscellaneousData = cleanedDS[['Daily Target','Unnamed: 14',\n",
    "                               'Sales Target', 'Unnamed: 19',\n",
    "                               'Branch.1', 'Unnamed: 24',\n",
    "                               'Unnamed: 25']]\n",
    "miscellaneousData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyTarget = miscellaneousData[[\"Daily Target\",\"Unnamed: 14\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyTarget.columns = [\"DailyTarget\",\"SalesTarget\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56846913",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyTarget.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyTarget = dailyTarget.iloc[1:,:]\n",
    "dailyTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e8ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "branchTarget = miscellaneousData[[\"Branch.1\",\"Unnamed: 24\",\"Unnamed: 25\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "branchTarget.columns = [\"Branch\",\"Manager\",\"Location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "branchTarget.dropna(axis=0,inplace=True)\n",
    "branctTarget = branchTarget.iloc[1:,:]\n",
    "branchTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee8611",
   "metadata": {},
   "outputs": [],
   "source": [
    "productTarget = miscellaneousData[['Sales Target', 'Unnamed: 19']]\n",
    "productTarget.columns = ['PizzaType','SalesTarget']\n",
    "productTarget = productTarget.iloc[1:]\n",
    "productTarget.dropna(axis=0,inplace=True)\n",
    "productTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d65d271",
   "metadata": {},
   "source": [
    "### Pyarrow is required to convert pandas DF to pyspark DF\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "saleDataSparkDF = ps.from_pandas(saleData)\n",
    "saleDataSparkDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc53de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is pyspark pandas variety of dataframe. \n",
    "#The regular Pyspark dataframe is sql variety. There are limits\n",
    "type(saleDataSparkDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(saleDataSparkDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f3bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions to work with the database\n",
    "def schemaGen(dataframe, schemaName):\n",
    "    localSchema = pd.io.sql.get_schema(dataframe,schemaName)\n",
    "    localSchema = localSchema.replace('TEXT','VARCHAR(255)').replace('INTEGER','NUMERIC').replace('\\n','').replace('\"',\"\")\n",
    "    return \"\".join(localSchema)\n",
    "\n",
    "#Using pandas read_sql for getting schema\n",
    "def getSchema(tableName, credentials):\n",
    "    schema = pd.read_sql(\"\"\"SELECT table_catalog, table_name, \n",
    "                column_name, data_type, \n",
    "                ordinal_position, column_default, character_maximum_length,\n",
    "                is_nullable FROM information_schema.columns where table_name='{}'\"\"\".format(tableName),con=credentials)\n",
    "    return schema\n",
    "\n",
    "#Issue is in using pd.read_sql to write data to the database. so using psycopg2\n",
    "def queryTable(query):\n",
    "    try:\n",
    "        schema = cur.execute(query)\n",
    "        return \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "#This doesn't return anything\n",
    "\n",
    "#Using the pd.read_sql for getting data from db\n",
    "def queryBase(query):\n",
    "    requiredTable = pd.read_sql(query,con=credentials)\n",
    "    return requiredTable\n",
    "\n",
    "#This returns the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaGen(saleData,'saleData')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd8827d",
   "metadata": {},
   "source": [
    "### Pandas to Pyspark DF looses some power\n",
    "\n",
    "Pyspark dataframe has sql related functions in-built into the object, while the pandas dataframe converted to pyspark df lacks this support.\n",
    "\n",
    "We can fall back to the pandas own sql modules, and learn about the schema, and use the python database connection library like psycopg2 and query in the data.\n",
    "\n",
    "Or, the Xlsx file can be converted to dataframe, then written out as csv files.Which then can be read into Pyspark context which has the database driver configured. \n",
    "\n",
    "There is mulitiple steps involved, so writing functions to do these two activities will save considerable effort. The same can be found in the next couple of cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c701e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "saleData.to_csv(\"dashBoard_saleData.csv\",index=False)\n",
    "branctTarget.to_csv(\"dashBoard_branches.csv\",index=False)\n",
    "dailyTarget.to_csv(\"dashBoard_dailyTarget.csv\",index=False)\n",
    "productTarget.to_csv(\"dashBoard_productTarget.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eed9bc2",
   "metadata": {},
   "source": [
    "Need to escape the special characters, when using OS commands inside Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296802fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls | grep dashBoard\\*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1efeb89",
   "metadata": {},
   "source": [
    "Re-ingest the data into the pyspark environment, and then write into the postgres database. Idea here is to create another spark session inside a function, and then load the csv file, then move it into the database. \n",
    "\n",
    "Here goes nothing...\n",
    "\n",
    "After experimenting a bit, I found it is better to initiate the session configured with the database jar files. Once the XL files are read, and cleaned using pandas, dataframe is written out into CSV file. \n",
    "\n",
    "The csv file path, the database name, database table name along with the spark session configured with the database driver, is used as parameters for the function, that writes to database. \n",
    "\n",
    "The same technique can be used for Spark dataframes also. In that case, the csv file will be replaced with spark dataframe itself. The database, tablename, and session will be still required. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a25eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 20:15:05 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 172.17.0.1 instead (on interface docker0)\n",
      "22/11/28 20:15:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/11/28 20:15:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "    \n",
    "sparkSQL = SparkSession.builder.appName('Spark SQL') \\\n",
    "        .config('spark.jars',\"/usr/share/java/postgresql-42.2.26.jar\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ad9907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writingCSVFiletoDatabase(session, csvFile,dbName,dbTableName):\n",
    "    \n",
    "    fileSparkDF = session.read.csv(csvFile,inferSchema=True,header=True)\n",
    "    try:\n",
    "        fileSparkDF.write \\\n",
    "                    .format('jdbc') \\\n",
    "                    .option(\"url\", f\"jdbc:postgresql://localhost:5432/{dbName}\") \\\n",
    "                    .option('dbtable', dbTableName) \\\n",
    "                    .option('user','postgres') \\\n",
    "                    .option('password', 1234) \\\n",
    "                    .option('driver','org.postgresql.Driver') \\\n",
    "                    .save(mode='overwrite')\n",
    "        print('Write Complete')\n",
    "    except Exception as e:\n",
    "        print(f'Write errored out due to {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4424088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writingSparkDFtoDatabase(session,sparkDF,dbName,dbTableName):\n",
    "    \n",
    "    try:\n",
    "        sparkDF.write \\\n",
    "                    .format('jdbc') \\\n",
    "                    .option(\"url\", f\"jdbc:postgresql://localhost:5432/{dbName}\") \\\n",
    "                    .option('dbtable', dbTableName) \\\n",
    "                    .option('user','postgres') \\\n",
    "                    .option('password', 1234) \\\n",
    "                    .option('driver','org.postgresql.Driver') \\\n",
    "                    .save(mode='overwrite')\n",
    "        print('Write Complete')\n",
    "    except Exception as e:\n",
    "        print(f'Write errored out due to {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6eddfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Complete\n"
     ]
    }
   ],
   "source": [
    "ritingCSVFiletoDatabase(sparkSQL,\"dashBoard_saleData.csv\",\"postgres\",\"sale_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba9550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
